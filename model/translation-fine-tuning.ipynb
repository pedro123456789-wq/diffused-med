{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedrolourenco/Documents/Web3/Nillion-Med-Translator/nillion-med-translator/model/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import numpy as np\n",
    "import time\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "books = load_dataset(\"opus_books\", \"en-fr\")\n",
    "\n",
    "# load model\n",
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=\"llm-tokenizer\")\n",
    "model = BertForMaskedLM.from_pretrained(model_name, cache_dir=\"llm-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, books, tokenizer, from_language='en', to_language='fr', max_length=128):\n",
    "        self.books = books\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.from_language = from_language\n",
    "        self.to_language = to_language\n",
    "        self.examples = []\n",
    "\n",
    "        self._prepare_examples()\n",
    "\n",
    "    def _prepare_examples(self):\n",
    "        # Generate incremental source-target pairs for each data pair\n",
    "        print(len(self.books['train']['translation']))\n",
    "        for pair in self.books['train']['translation'][:10000]:\n",
    "            source_text = pair[self.from_language]\n",
    "            target_text = pair[self.to_language]\n",
    "            combined_text = \"SOURCE: \" + source_text + \"; TRANSLATION: \" + target_text\n",
    "\n",
    "            # Tokenize the entire combined text once\n",
    "            encoding = self.tokenizer(\n",
    "                combined_text,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].flatten()\n",
    "\n",
    "            # Create incremental source-target pairs\n",
    "            for i in range(1, len(input_ids)):\n",
    "                # Create prefix up to the i-th token\n",
    "                input_prefix = input_ids[:i]\n",
    "                target_token = input_ids[i]\n",
    "\n",
    "                # Only add pairs if within max length\n",
    "                if len(input_prefix) < self.max_length:\n",
    "                    # Pad input_prefix to max_length\n",
    "                    padded_input = torch.cat([\n",
    "                        input_prefix,\n",
    "                        torch.full((self.max_length - len(input_prefix),), self.tokenizer.pad_token_id)\n",
    "                    ])\n",
    "\n",
    "                    # Prepare label for next token prediction\n",
    "                    label = torch.full((self.max_length,), -100)\n",
    "                    label[i] = target_token  # Only the next token is the target\n",
    "\n",
    "                    # Add example to dataset\n",
    "                    self.examples.append({\n",
    "                        'input_ids': padded_input,\n",
    "                        'attention_mask': (padded_input != self.tokenizer.pad_token_id).long(),\n",
    "                        'labels': label\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CustomDataset at 0x1579cbc20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = train_dataset = CustomDataset(books, tokenizer)\n",
    "dataloader = DataLoader(\n",
    "        train_dataset, batch_size=32, shuffle=True, num_workers=0\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (30522) must match the size of tensor b (128) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime taken for epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 34\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, num_epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m     33\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m right_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mpreds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     37\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (30522) must match the size of tensor b (128) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=3):\n",
    "    # Initialize optimizer only with parameters that require gradients\n",
    "    optimizer = AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=1e-5\n",
    "    )\n",
    "\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        right_predictions = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "        \n",
    "            model.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            # Calculate accuracy\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            right_predictions += torch.sum(preds == labels).item()\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}:')\n",
    "        print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "        print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "        print(f'Right predictions: {right_predictions} out of {len(train_loader) * 32}')\n",
    "        print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "        print(f'Time taken for epoch: {end_time - start_time:.2f} seconds')\n",
    "        print('-' * 60)\n",
    "\n",
    "train_model(model, dataloader, dataloader, \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
