{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"(Vertigo) Paroxysmal Positional Vertigo\", \n",
    "    \"AIDS\", \n",
    "    \"Acne\", \n",
    "    \"Alcoholic hepatitis\", \n",
    "    \"Allergy\", \n",
    "    \"Arthritis\", \n",
    "    \"Bronchial Asthma\", \n",
    "    \"Cervical spondylosis\", \n",
    "    \"Chicken pox\", \n",
    "    \"Chronic cholestasis\", \n",
    "    \"Common Cold\", \n",
    "    \"Dengue\", \n",
    "    \"Diabetes\", \n",
    "    \"Dimorphic hemorrhoids (piles)\",\n",
    "    \"Drug Reaction\",\n",
    "    \"Fungal infection\",\n",
    "    \"GERD\",\n",
    "    \"Gastroenteritis\",\n",
    "    \"Heart attack\",\n",
    "    \"Hepatitis B\",\n",
    "    \"Hepatitis C\",\n",
    "    \"Hepatitis D\",\n",
    "    \"Hepatitis E\",\n",
    "    \"Hypertension\",\n",
    "    \"Hyperthyroidism\",\n",
    "    \"Hypoglycemia\",\n",
    "    \"Hypothyroidism\",\n",
    "    \"Impetigo\",\n",
    "    \"Jaundice\",\n",
    "    \"Malaria\",\n",
    "    \"Migraine\",\n",
    "    \"Osteoarthritis\",\n",
    "    \"Paralysis (brain hemorrhage)\",\n",
    "    \"Peptic ulcer disease\",\n",
    "    \"Pneumonia\",\n",
    "    \"Psoriasis\",\n",
    "    \"Tuberculosis\",\n",
    "    \"Typhoid\",\n",
    "    \"Urinary tract infection\",\n",
    "    \"Varicose veins\",\n",
    "    \"Hepatitis A\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedrolourenco/Documents/Web3/Nillion-Med-Translator/nillion-med-translator/model/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosis Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedrolourenco/Documents/Web3/Nillion-Med-Translator/nillion-med-translator/model/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 18\n",
      "Class probabilities: tensor([[7.4849e-03, 9.9875e-04, 4.6661e-03, 3.0691e-03, 6.3025e-03, 2.6628e-03,\n",
      "         2.0095e-03, 4.3723e-03, 4.6710e-03, 1.7938e-03, 5.2376e-03, 3.1679e-03,\n",
      "         2.3369e-03, 7.2384e-04, 3.6365e-03, 5.2172e-03, 1.6723e-02, 9.7272e-03,\n",
      "         7.5583e-01, 1.5773e-03, 1.5026e-03, 9.3692e-03, 7.2156e-03, 5.1868e-02,\n",
      "         3.1464e-03, 1.5201e-02, 2.9604e-03, 3.6804e-03, 5.1570e-03, 7.9963e-03,\n",
      "         2.1252e-03, 2.3563e-03, 2.4780e-03, 2.2738e-03, 7.7190e-03, 4.0139e-03,\n",
      "         2.0764e-03, 1.3460e-02, 3.5330e-03, 4.7164e-03, 4.9413e-03]])\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shanover/symps_disease_bert_v3_c41\", cache_dir=\"./llm-tokeniser\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"shanover/symps_disease_bert_v3_c41\", cache_dir=\"./llm-model\")\n",
    "\n",
    "# Input text\n",
    "text = \"I am feeling vomiting, breathlessness, and sweating\"\n",
    "\n",
    "# Tokenize and prepare input for the model\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=1)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "def convert_pytorch_to_onnx_with_tokenizer(model, tokenizer, max_length=128, onnx_file_path=None):\n",
    "    \"\"\"\n",
    "    Converts a PyTorch model to ONNX format, using tokenizer output as input.\n",
    "\n",
    "    Args:\n",
    "    model (torch.nn.Module): The PyTorch model to be converted.\n",
    "    tokenizer: The tokenizer used to preprocess the input.\n",
    "    onnx_file_path (str): The file path where the ONNX model will be saved.\n",
    "    max_length (int): Maximum sequence length for the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare dummy input using the tokenizer\n",
    "    dummy_input = \"This is a sample input text for ONNX conversion.\"\n",
    "    inputs = tokenizer(\n",
    "        dummy_input,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the input names\n",
    "    input_names = list(inputs.keys())\n",
    "    input_names = [\"input_ids\", \"attention_mask\"]\n",
    "    print(f\"Input names: {input_names}\")\n",
    "\n",
    "    # # Create dummy inputs for ONNX export\n",
    "    # dummy_inputs = tuple(encoded_input[name] for name in input_names)\n",
    "    dynamic_axes = {name: {0: \"batch_size\"} for name in input_names}\n",
    "    dynamic_axes.update({f\"logits\": {0: \"batch_size\"}})\n",
    "    print(f\"dynamic_axes: {dynamic_axes}\")\n",
    "    \n",
    "    # Export the model\n",
    "    torch.onnx.export(\n",
    "        model,  # model being run\n",
    "        tuple(inputs[k] for k in input_names),  # model inputs\n",
    "        onnx_file_path,  # where to save the model\n",
    "        export_params=True,  # store the trained parameter weights inside the model file\n",
    "        opset_version=20,  # the ONNX version to export the model to\n",
    "        do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "        input_names=input_names,  # the model's input names\n",
    "        output_names=[\"logits\"],  # the model's output names\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )  # variable length axes\n",
    "\n",
    "    print(f\"Model exported to {onnx_file_path}\")\n",
    "\n",
    "    # Verify the exported model\n",
    "    onnx_model = onnx.load(onnx_file_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"ONNX model is valid.\")\n",
    "    return onnx_file_path, input_names\n",
    "\n",
    "\n",
    "# convert_pytorch_to_onnx_with_tokenizer(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at medicalai/ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"medicalai/ClinicalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"./classifier-tokenizer\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(classes), \n",
    "    cache_dir=\"./classifier-model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input names: ['input_ids', 'attention_mask']\n",
      "dynamic_axes: {'input_ids': {0: 'batch_size'}, 'attention_mask': {0: 'batch_size'}, 'logits': {0: 'batch_size'}}\n",
      "Model exported to ./saved-models/diagnosis-classifier.onnx\n",
      "ONNX model is valid.\n"
     ]
    }
   ],
   "source": [
    "onnx_file_path, input_names = convert_pytorch_to_onnx_with_tokenizer(\n",
    "        model, tokenizer, max_length=128, onnx_file_path=\"./saved-models/diagnosis-classifier.onnx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BertTinySMS', 'LeNet5MNIST']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import aivm_client as aic\n",
    "aic.get_supported_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aivm_client as aic\n",
    "aic.upload_bert_tiny_model(\"./saved-models/diagnosis-classifier.onnx\", \"diagnosis-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m tokenized_input \u001b[38;5;241m=\u001b[39m aic\u001b[38;5;241m.\u001b[39mtokenize(input_text,)\n\u001b[1;32m      4\u001b[0m encrypted_input \u001b[38;5;241m=\u001b[39m aic\u001b[38;5;241m.\u001b[39mBertTinyCryptensor(tokenized_input[\u001b[38;5;241m0\u001b[39m], tokenized_input[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43maic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencrypted_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiagnosis-classifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m result\n",
      "File \u001b[0;32m~/Documents/Web3/Nillion-Med-Translator/nillion-med-translator/model/env/lib/python3.12/site-packages/aivm_client/client.py:70\u001b[0m, in \u001b[0;36mget_prediction\u001b[0;34m(inputs, model, model_type)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mGet predictions from a model using encrypted tensors.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    ValueError: If inputs are not of a supported cryptensor type.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, BertTinyCryptensor):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcurl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBertTiny\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, LeNet5Cryptensor):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m curl\u001b[38;5;241m.\u001b[39mget_prediction(inputs, model, curl\u001b[38;5;241m.\u001b[39mModelType\u001b[38;5;241m.\u001b[39mLeNet5)\n",
      "File \u001b[0;32m~/Documents/Web3/Nillion-Med-Translator/nillion-med-translator/model/env/lib/python3.12/site-packages/curl_client/__init__.py:118\u001b[0m, in \u001b[0;36mget_prediction\u001b[0;34m(tensor, model, model_type)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_prediction\u001b[39m(tensor, model, model_type):\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    Sends shares to the server using the provided gRPC stub.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m        shares: The shares to be sent\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCLIENT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Web3/Nillion-Med-Translator/nillion-med-translator/model/env/lib/python3.12/site-packages/curl_client/__init__.py:63\u001b[0m, in \u001b[0;36mCurlClient.get_prediction\u001b[0;34m(self, tensor, model, model_type)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, (ArithmeticSharedTensor, BinarySharedTensor)):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput tensor must be a CrypTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m shared_tensor \u001b[38;5;241m=\u001b[39m from_shares(\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshares\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_bits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m shared_tensor\u001b[38;5;241m.\u001b[39mreconstruct()\n",
      "File \u001b[0;32m~/Documents/Web3/Nillion-Med-Translator/nillion-med-translator/model/env/lib/python3.12/site-packages/curl_client/network.py:57\u001b[0m, in \u001b[0;36mNetworkManager.get_prediction\u001b[0;34m(self, shares, model, precision_bits, model_type)\u001b[0m\n\u001b[1;32m     55\u001b[0m messages \u001b[38;5;241m=\u001b[39m sspb2\u001b[38;5;241m.\u001b[39mClientShares(shares\u001b[38;5;241m=\u001b[39mshare_messages)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     59\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(e)\n",
      "File \u001b[0;32m~/Documents/Web3/Nillion-Med-Translator/nillion-med-translator/model/env/lib/python3.12/site-packages/grpc/_channel.py:1178\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1175\u001b[0m     (\n\u001b[1;32m   1176\u001b[0m         state,\n\u001b[1;32m   1177\u001b[0m         call,\n\u001b[0;32m-> 1178\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Web3/Nillion-Med-Translator/nillion-med-translator/model/env/lib/python3.12/site-packages/grpc/_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[1;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[1;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[0;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#running inference secure inference\n",
    "input_text = \"I am feeling vomiting, breathlessness, and sweating\"\n",
    "tokenized_input = aic.tokenize(input_text,)\n",
    "encrypted_input = aic.BertTinyCryptensor(tokenized_input[0], tokenized_input[1])\n",
    "result = aic.get_prediction(encrypted_input, \"diagnosis-classifier\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload model\n",
    "MODEL_NAME = \"diagnosis-classifier\" # Name of the model to be used\n",
    "aic.upload_bert_tiny_model(\"./saved_models/diagnosis_classifier.onnx\", MODEL_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predicted tokens and their probabilities:\n",
      "Token: today - Probability: 0.1905\n",
      "Token: . - Probability: 0.1574\n",
      "Token: : - Probability: 0.1569\n",
      "Token: the - Probability: 0.0707\n",
      "Token: , - Probability: 0.0409\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Load the BERT Tiny model and tokenizer\n",
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Sample input text\n",
    "input_text = \"The weather today is\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass to get the logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # Shape: [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "# Get probabilities for each token in the vocabulary at each position\n",
    "probabilities = softmax(logits, dim=-1)  # Shape: [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "# Example: Probability distribution for the last token\n",
    "last_token_prob = probabilities[0, -1, :]  # Probability distribution over vocabulary for the last token\n",
    "top_5_tokens = torch.topk(last_token_prob, 5)  # Get top 5 tokens with highest probabilities\n",
    "\n",
    "# Convert token IDs to words\n",
    "top_5_tokens_words = [tokenizer.decode([token_id]) for token_id in top_5_tokens.indices]\n",
    "top_5_probabilities = top_5_tokens.values.tolist()\n",
    "\n",
    "# Output the results\n",
    "print(\"Top 5 predicted tokens and their probabilities:\")\n",
    "for word, prob in zip(top_5_tokens_words, top_5_probabilities):\n",
    "    print(f\"Token: {word} - Probability: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
