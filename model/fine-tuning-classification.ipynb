{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#use GPU on mac if enabled\n",
    "if torch.backends.mps.is_available():\n",
    "    print(f\"GPU is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"drug reaction\",\n",
    "    \"allergy\",\n",
    "    \"chicken pox\",\n",
    "    \"diabetes\",\n",
    "    \"psoriasis\",\n",
    "    \"hypertension\",\n",
    "    \"cervical spondylosis\",\n",
    "    \"bronchial asthma\",\n",
    "    \"varicose veins\",\n",
    "    \"malaria\",\n",
    "    \"dengue\",\n",
    "    \"arthritis\",\n",
    "    \"impetigo\",\n",
    "    \"fungal infection\",\n",
    "    \"common cold\",\n",
    "    \"gastroesophageal reflux disease\",\n",
    "    \"urinary tract infection\",\n",
    "    \"typhoid\",\n",
    "    \"pneumonia\",\n",
    "    \"peptic ulcer disease\",\n",
    "    \"jaundice\",\n",
    "    \"migraine\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom torch dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels.index(label), dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_bert_layers(model):\n",
    "    \"\"\"\n",
    "    Freezes all bert layers apart from the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    for param in model.bert.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for layer in model.bert.encoder.layer[:-1]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    #show information about trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=3):\n",
    "    #initialise optimiser only with parameters that require gradients\n",
    "    optimizer = AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=1e-5\n",
    "    )\n",
    "\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        right_predictions = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "        \n",
    "            model.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            #calculate accuracy\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            right_predictions += torch.sum(preds == labels).item()\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}:')\n",
    "        print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "        print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "        print(f'Right predictions: {right_predictions} out of {len(train_loader) * 32}')\n",
    "        print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "        print(f'Time taken for epoch: {end_time - start_time:.2f} seconds')\n",
    "        print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, onnx\n",
    "\n",
    "def convert_pytorch_to_onnx_with_tokenizer(model, tokenizer, max_length=128, onnx_file_path=None):\n",
    "    \"\"\"\n",
    "    Converts a PyTorch model to ONNX format, using tokenizer output as input.\n",
    "\n",
    "    Args:\n",
    "    model (torch.nn.Module): The PyTorch model to be converted.\n",
    "    tokenizer: The tokenizer used to preprocess the input.\n",
    "    onnx_file_path (str): The file path where the ONNX model will be saved.\n",
    "    max_length (int): Maximum sequence length for the tokenizer.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare dummy input using the tokenizer\n",
    "    dummy_input = \"This is a sample input text for ONNX conversion.\"\n",
    "    inputs = tokenizer(\n",
    "        dummy_input,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # # Get the input names\n",
    "    input_names = list(inputs.keys())\n",
    "    input_names = [\"input_ids\", \"attention_mask\"]\n",
    "    print(f\"Input names: {input_names}\")\n",
    "\n",
    "    # # Create dummy inputs for ONNX export\n",
    "    # dummy_inputs = tuple(encoded_input[name] for name in input_names)\n",
    "    if onnx_file_path is None:\n",
    "      onnx_file_path = tempfile.mktemp(suffix=\".onnx\")\n",
    "    dynamic_axes = {name: {0: \"batch_size\"} for name in input_names}\n",
    "    dynamic_axes.update({f\"logits\": {0: \"batch_size\"}})\n",
    "    print(f\"dynamic_axes: {dynamic_axes}\")\n",
    "    # Export the model\n",
    "    torch.onnx.export(\n",
    "        model,  # model being run\n",
    "        tuple(inputs[k] for k in input_names),  # model inputs\n",
    "        onnx_file_path,  # where to save the model\n",
    "        export_params=True,  # store the trained parameter weights inside the model file\n",
    "        opset_version=20,  # the ONNX version to export the model to\n",
    "        do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "        input_names=input_names,  # the model's input names\n",
    "        output_names=[\"logits\"],  # the model's output names\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )  # variable length axes\n",
    "\n",
    "    print(f\"Model exported to {onnx_file_path}\")\n",
    "\n",
    "    # Verify the exported model\n",
    "    onnx_model = onnx.load(onnx_file_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"ONNX model is valid.\")\n",
    "    return onnx_file_path, input_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4,388,758\n",
      "Trainable parameters: 217,622\n",
      "Percentage of trainable parameters: 4.96%\n"
     ]
    }
   ],
   "source": [
    "# load tiny bert model and tokenizer\n",
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(labels)\n",
    ")\n",
    "freeze_bert_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedrolourenco/Documents/Web3/Nillion-Med-Translator/nillion-med-translator/model/env/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Average training loss: 1.6183\n",
      "Average validation loss: 1.5438\n",
      "Right predictions: 592 out of 864\n",
      "Validation Accuracy: 0.6557\n",
      "Time taken for epoch: 1.64 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 2:\n",
      "Average training loss: 1.5952\n",
      "Average validation loss: 1.5378\n",
      "Right predictions: 592 out of 864\n",
      "Validation Accuracy: 0.6651\n",
      "Time taken for epoch: 1.65 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 3:\n",
      "Average training loss: 1.6063\n",
      "Average validation loss: 1.5303\n",
      "Right predictions: 594 out of 864\n",
      "Validation Accuracy: 0.6745\n",
      "Time taken for epoch: 1.68 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 4:\n",
      "Average training loss: 1.5951\n",
      "Average validation loss: 1.5249\n",
      "Right predictions: 600 out of 864\n",
      "Validation Accuracy: 0.6651\n",
      "Time taken for epoch: 1.72 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 5:\n",
      "Average training loss: 1.5971\n",
      "Average validation loss: 1.5211\n",
      "Right predictions: 594 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.78 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 6:\n",
      "Average training loss: 1.5831\n",
      "Average validation loss: 1.5123\n",
      "Right predictions: 594 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.69 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 7:\n",
      "Average training loss: 1.5789\n",
      "Average validation loss: 1.5101\n",
      "Right predictions: 600 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.71 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 8:\n",
      "Average training loss: 1.5791\n",
      "Average validation loss: 1.5068\n",
      "Right predictions: 582 out of 864\n",
      "Validation Accuracy: 0.6745\n",
      "Time taken for epoch: 1.79 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 9:\n",
      "Average training loss: 1.5707\n",
      "Average validation loss: 1.5000\n",
      "Right predictions: 588 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.82 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 10:\n",
      "Average training loss: 1.5784\n",
      "Average validation loss: 1.4927\n",
      "Right predictions: 588 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.75 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 11:\n",
      "Average training loss: 1.5572\n",
      "Average validation loss: 1.4885\n",
      "Right predictions: 595 out of 864\n",
      "Validation Accuracy: 0.6651\n",
      "Time taken for epoch: 1.70 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 12:\n",
      "Average training loss: 1.5625\n",
      "Average validation loss: 1.4845\n",
      "Right predictions: 598 out of 864\n",
      "Validation Accuracy: 0.6651\n",
      "Time taken for epoch: 1.69 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 13:\n",
      "Average training loss: 1.5571\n",
      "Average validation loss: 1.4805\n",
      "Right predictions: 594 out of 864\n",
      "Validation Accuracy: 0.6651\n",
      "Time taken for epoch: 1.70 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 14:\n",
      "Average training loss: 1.5464\n",
      "Average validation loss: 1.4760\n",
      "Right predictions: 601 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.70 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 15:\n",
      "Average training loss: 1.5508\n",
      "Average validation loss: 1.4734\n",
      "Right predictions: 605 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.70 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 16:\n",
      "Average training loss: 1.5367\n",
      "Average validation loss: 1.4683\n",
      "Right predictions: 603 out of 864\n",
      "Validation Accuracy: 0.6651\n",
      "Time taken for epoch: 1.71 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 17:\n",
      "Average training loss: 1.5272\n",
      "Average validation loss: 1.4642\n",
      "Right predictions: 605 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.71 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 18:\n",
      "Average training loss: 1.5273\n",
      "Average validation loss: 1.4605\n",
      "Right predictions: 603 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.71 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 19:\n",
      "Average training loss: 1.5197\n",
      "Average validation loss: 1.4570\n",
      "Right predictions: 619 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.76 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 20:\n",
      "Average training loss: 1.5217\n",
      "Average validation loss: 1.4534\n",
      "Right predictions: 609 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.71 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 21:\n",
      "Average training loss: 1.5182\n",
      "Average validation loss: 1.4504\n",
      "Right predictions: 610 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.72 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 22:\n",
      "Average training loss: 1.5193\n",
      "Average validation loss: 1.4460\n",
      "Right predictions: 610 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.74 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 23:\n",
      "Average training loss: 1.5221\n",
      "Average validation loss: 1.4457\n",
      "Right predictions: 597 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.72 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 24:\n",
      "Average training loss: 1.5094\n",
      "Average validation loss: 1.4429\n",
      "Right predictions: 607 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.72 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 25:\n",
      "Average training loss: 1.5005\n",
      "Average validation loss: 1.4395\n",
      "Right predictions: 614 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.73 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 26:\n",
      "Average training loss: 1.5015\n",
      "Average validation loss: 1.4354\n",
      "Right predictions: 586 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.72 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 27:\n",
      "Average training loss: 1.5052\n",
      "Average validation loss: 1.4328\n",
      "Right predictions: 606 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.73 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 28:\n",
      "Average training loss: 1.5053\n",
      "Average validation loss: 1.4312\n",
      "Right predictions: 604 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.75 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 29:\n",
      "Average training loss: 1.5064\n",
      "Average validation loss: 1.4284\n",
      "Right predictions: 585 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.77 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 30:\n",
      "Average training loss: 1.4856\n",
      "Average validation loss: 1.4263\n",
      "Right predictions: 615 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.75 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 31:\n",
      "Average training loss: 1.4918\n",
      "Average validation loss: 1.4240\n",
      "Right predictions: 615 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.73 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 32:\n",
      "Average training loss: 1.4923\n",
      "Average validation loss: 1.4214\n",
      "Right predictions: 606 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.74 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 33:\n",
      "Average training loss: 1.4947\n",
      "Average validation loss: 1.4199\n",
      "Right predictions: 619 out of 864\n",
      "Validation Accuracy: 0.6698\n",
      "Time taken for epoch: 1.72 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 34:\n",
      "Average training loss: 1.4712\n",
      "Average validation loss: 1.4179\n",
      "Right predictions: 603 out of 864\n",
      "Validation Accuracy: 0.6745\n",
      "Time taken for epoch: 1.74 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 35:\n",
      "Average training loss: 1.4861\n",
      "Average validation loss: 1.4156\n",
      "Right predictions: 607 out of 864\n",
      "Validation Accuracy: 0.6792\n",
      "Time taken for epoch: 1.72 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 36:\n",
      "Average training loss: 1.4778\n",
      "Average validation loss: 1.4142\n",
      "Right predictions: 618 out of 864\n",
      "Validation Accuracy: 0.6840\n",
      "Time taken for epoch: 1.73 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 37:\n",
      "Average training loss: 1.4583\n",
      "Average validation loss: 1.4135\n",
      "Right predictions: 632 out of 864\n",
      "Validation Accuracy: 0.6745\n",
      "Time taken for epoch: 1.76 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 38:\n",
      "Average training loss: 1.4853\n",
      "Average validation loss: 1.4118\n",
      "Right predictions: 606 out of 864\n",
      "Validation Accuracy: 0.6887\n",
      "Time taken for epoch: 1.75 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 39:\n",
      "Average training loss: 1.4775\n",
      "Average validation loss: 1.4102\n",
      "Right predictions: 609 out of 864\n",
      "Validation Accuracy: 0.6887\n",
      "Time taken for epoch: 1.76 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 40:\n",
      "Average training loss: 1.4727\n",
      "Average validation loss: 1.4089\n",
      "Right predictions: 599 out of 864\n",
      "Validation Accuracy: 0.6887\n",
      "Time taken for epoch: 1.79 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 41:\n",
      "Average training loss: 1.4667\n",
      "Average validation loss: 1.4078\n",
      "Right predictions: 608 out of 864\n",
      "Validation Accuracy: 0.6887\n",
      "Time taken for epoch: 1.77 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 42:\n",
      "Average training loss: 1.4748\n",
      "Average validation loss: 1.4066\n",
      "Right predictions: 608 out of 864\n",
      "Validation Accuracy: 0.6887\n",
      "Time taken for epoch: 1.76 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 43:\n",
      "Average training loss: 1.4686\n",
      "Average validation loss: 1.4056\n",
      "Right predictions: 622 out of 864\n",
      "Validation Accuracy: 0.6887\n",
      "Time taken for epoch: 1.79 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 44:\n",
      "Average training loss: 1.4757\n",
      "Average validation loss: 1.4050\n",
      "Right predictions: 613 out of 864\n",
      "Validation Accuracy: 0.6887\n",
      "Time taken for epoch: 1.76 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 45:\n",
      "Average training loss: 1.4710\n",
      "Average validation loss: 1.4045\n",
      "Right predictions: 616 out of 864\n",
      "Validation Accuracy: 0.6887\n",
      "Time taken for epoch: 1.79 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 46:\n",
      "Average training loss: 1.4680\n",
      "Average validation loss: 1.4040\n",
      "Right predictions: 626 out of 864\n",
      "Validation Accuracy: 0.6887\n",
      "Time taken for epoch: 1.77 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 47:\n",
      "Average training loss: 1.4775\n",
      "Average validation loss: 1.4035\n",
      "Right predictions: 606 out of 864\n",
      "Validation Accuracy: 0.6887\n",
      "Time taken for epoch: 1.79 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 48:\n",
      "Average training loss: 1.4685\n",
      "Average validation loss: 1.4033\n",
      "Right predictions: 615 out of 864\n",
      "Validation Accuracy: 0.6840\n",
      "Time taken for epoch: 1.87 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 49:\n",
      "Average training loss: 1.4652\n",
      "Average validation loss: 1.4031\n",
      "Right predictions: 629 out of 864\n",
      "Validation Accuracy: 0.6887\n",
      "Time taken for epoch: 1.76 seconds\n",
      "------------------------------------------------------------\n",
      "Epoch 50:\n",
      "Average training loss: 1.4537\n",
      "Average validation loss: 1.4030\n",
      "Right predictions: 607 out of 864\n",
      "Validation Accuracy: 0.6887\n",
      "Time taken for epoch: 1.76 seconds\n",
      "------------------------------------------------------------\n",
      "Input names: ['input_ids', 'attention_mask']\n",
      "dynamic_axes: {'input_ids': {0: 'batch_size'}, 'attention_mask': {0: 'batch_size'}, 'logits': {0: 'batch_size'}}\n",
      "Model exported to ./saved-models/diagnosis-classifier.onnx\n",
      "ONNX model is valid.\n",
      "ONNX file path: ./saved-models/diagnosis-classifier.onnx\n",
      "Input names: ['input_ids', 'attention_mask']\n",
      "torch.Size([22])\n",
      "Text: My head hurts and I have a high temperature.\n",
      "drug reaction: 0.06444866210222244\n",
      "allergy: 0.048033446073532104\n",
      "chicken pox: 0.04502548277378082\n",
      "diabetes: 0.042104899883270264\n",
      "psoriasis: 0.04214897006750107\n",
      "hypertension: 0.006122661288827658\n",
      "cervical spondylosis: 0.012386810034513474\n",
      "bronchial asthma: 0.014559955336153507\n",
      "varicose veins: 0.023168375715613365\n",
      "malaria: 0.0039571975357830524\n",
      "dengue: 0.05196285992860794\n",
      "arthritis: 0.005639791488647461\n",
      "impetigo: 0.038125380873680115\n",
      "fungal infection: 0.04790700972080231\n",
      "common cold: 0.023025652393698692\n",
      "gastroesophageal reflux disease: 0.1933353692293167\n",
      "urinary tract infection: 0.10679526627063751\n",
      "typhoid: 0.02999836765229702\n",
      "pneumonia: 0.011642919853329659\n",
      "peptic ulcer disease: 0.12996263802051544\n",
      "jaundice: 0.042867571115493774\n",
      "migraine: 0.016780823469161987\n",
      "======\n",
      "torch.Size([22])\n",
      "Text: I have a red rash and shortness of breath\n",
      "drug reaction: 0.05240565538406372\n",
      "allergy: 0.05197979509830475\n",
      "chicken pox: 0.046927884221076965\n",
      "diabetes: 0.03883945569396019\n",
      "psoriasis: 0.07146180421113968\n",
      "hypertension: 0.023940706625580788\n",
      "cervical spondylosis: 0.017738359048962593\n",
      "bronchial asthma: 0.013882945291697979\n",
      "varicose veins: 0.020155813544988632\n",
      "malaria: 0.008000600151717663\n",
      "dengue: 0.06283793598413467\n",
      "arthritis: 0.004098458215594292\n",
      "impetigo: 0.213608518242836\n",
      "fungal infection: 0.10039456188678741\n",
      "common cold: 0.014745415188372135\n",
      "gastroesophageal reflux disease: 0.038250233978033066\n",
      "urinary tract infection: 0.03190387040376663\n",
      "typhoid: 0.016298606991767883\n",
      "pneumonia: 0.008503703400492668\n",
      "peptic ulcer disease: 0.03711634874343872\n",
      "jaundice: 0.06418260931968689\n",
      "migraine: 0.06272680312395096\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def main(model, tokenizer):\n",
    "    # load symptom to diagnosis dataset\n",
    "    dataset = load_dataset(\"gretelai/symptom_to_diagnosis\")\n",
    "\n",
    "    # Prepare train and validation datasets\n",
    "    train_texts = dataset['train']['input_text']\n",
    "    train_labels = dataset['train']['output_text']\n",
    "    val_texts = dataset['test']['input_text']\n",
    "    val_labels = dataset['test']['output_text']\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=32, shuffle=True, num_workers=0\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=32, num_workers=0\n",
    "    )\n",
    "\n",
    "    # Set device and move data to GPU if available\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, val_loader, device, num_epochs=50)\n",
    "\n",
    "    # Make model tensors contiguous and move to CPU before saving\n",
    "    model = model.cpu()\n",
    "\n",
    "    # Save the fine-tuned model as an ONNX file\n",
    "    onnx_file_path, input_names = convert_pytorch_to_onnx_with_tokenizer(\n",
    "        model, tokenizer, max_length=128, onnx_file_path=\"./saved-models/diagnosis-classifier.onnx\"\n",
    "    )\n",
    "    print(f\"ONNX file path: {onnx_file_path}\")\n",
    "    print(f\"Input names: {input_names}\")\n",
    "\n",
    "    # Test the model on a few examples\n",
    "    model.eval()\n",
    "    test_texts = [\n",
    "        \"My head hurts and I have a high temperature.\",\n",
    "        \"I have a red rash and shortness of breath\"\n",
    "    ]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            test_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        model.to(device)\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "\n",
    "        for text, pred in zip(test_texts, predictions):\n",
    "            print(pred.shape)\n",
    "            print(f\"Text: {text}\")\n",
    "            for i in range(0, len(pred)):\n",
    "                print(f'{labels[i]}: {pred[i]}')\n",
    "            print(\"======\")\n",
    "\n",
    "main(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload trained model to the nillion blockchain\n",
    "import aivm_client as aic\n",
    "\n",
    "MODEL_NAME = \"DIAGNOSIS_CLASSIFIER\" \n",
    "aic.upload_bert_tiny_model(\"./saved-models/diagnosis-classifier.onnx\", MODEL_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5g/d6hmjdh9523f48yxj6msdscm0000gn/T/ipykernel_10032/925244536.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = torch.nn.functional.softmax(result[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0389, 0.0536, 0.0508, 0.0425, 0.0582, 0.0366, 0.0443, 0.0407, 0.0429,\n",
       "        0.0461, 0.0432, 0.0422, 0.0450, 0.0468, 0.0459, 0.0599, 0.0433, 0.0451,\n",
       "        0.0467, 0.0389, 0.0497, 0.0387])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perform secure inference\n",
    "inp_text = \"I have a sore throat and the back of my head hurts\"\n",
    "tokens = aic.tokenize(inp_text,)\n",
    "encrypted_tokens = aic.BertTinyCryptensor(*tokens)\n",
    "result = aic.get_prediction(encrypted_tokens, MODEL_NAME)\n",
    "probs = torch.nn.functional.softmax(result[0])\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
